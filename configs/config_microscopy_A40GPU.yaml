defaults:
  - model: unet
  - data: real_pairs
  - physics: microscopy
  - guidance: pkl
  - training: ddpm
  - override hydra/launcher: basic
  - _self_

# Experiment configuration
experiment:
  name: microscopy_A40GPU_${now:%Y-%m-%d_%H-%M-%S}
  seed: 42
  device: cuda  # auto, cuda, cpu
  mixed_precision: true  # Use mixed precision for better memory efficiency

# Path configuration - Local server paths
paths:
  root: ${oc.env:PROJECT_ROOT,.}
  data: ${paths.root}/data
  checkpoints: ${paths.root}/checkpoints
  outputs: ${paths.root}/outputs
  logs: ${paths.root}/logs

# Weights & Biases logging
wandb:
  project: pkl-diffusion-microscopy-A40
  entity: anna_yoon-uc-berkeley  # Auto-configured from system credentials
  mode: offline  # Default to offline for local server

# Model configuration - Standard DDPM architecture for high-quality results
model:
  sample_size: 256                # Standard DDPM resolution (upgradeable to 512)
  in_channels: 1                   # Will be set to 2 automatically if use_conditioning=true
  out_channels: 1
  layers_per_block: 2
  block_out_channels: [64, 128, 256, 512]  # Standard DDPM channel progression
  down_block_types: ["DownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D"]  # More attention layers
  up_block_types: ["AttnUpBlock2D", "AttnUpBlock2D", "AttnUpBlock2D", "UpBlock2D"]            # More attention layers
  class_embed_type: null           # Can be enabled for conditional generation
  cross_attention_dim: null        # Can be enabled for text/class conditioning
  # Advanced Multi-resolution Training (Research-Grade)
  multi_resolution:
    enabled: true
    strategy: "hierarchical"        # hierarchical, progressive, cascaded, mixed
    
    # Resolution schedule - More sophisticated progression
    resolutions: [64, 128, 256, 512]  # Full resolution pyramid
    base_resolution: 64             # Starting resolution
    target_resolution: 256          # Final target resolution (can scale to 512)
    
    # Progressive training schedule
    schedule:
      type: "adaptive"              # adaptive, fixed, exponential
      transition_epochs: [30, 50, 80]  # Epochs to transition between resolutions
      warmup_epochs: 10             # Epochs to stabilize at each resolution
      overlap_epochs: 5             # Epochs of mixed-resolution training
    
    # Advanced features
    smooth_transitions: true        # Smooth alpha-blending between resolutions
    resolution_dropout: 0.1         # Randomly drop to lower resolution (regularization)
    adaptive_batch_scaling: true    # Scale batch size with resolution
    
    # Quality enhancements
    super_resolution_loss: true     # Additional loss for upsampling quality
    perceptual_consistency: true    # Ensure perceptual consistency across scales
    frequency_analysis: true        # Monitor frequency domain learning
    
    # Batch size scaling with resolution
    batch_scaling:
      64: 64                        # 64x64: batch_size * 4
      128: 32                       # 128x128: batch_size * 2  
      256: 16                       # 256x256: batch_size * 1
      512: 8                        # 512x512: batch_size * 0.5

# Training configuration - Optimized for single A40 GPU (46GB VRAM)
training:
  # Core training parameters
  max_epochs: 1000              # Maximum training epochs (will stop early if converged)
  num_timesteps: 1000           # Diffusion timesteps (1000 is optimal for quality)
  beta_schedule: cosine         # cosine schedule works better than linear
  learning_rate: 1.0e-4         # Stable learning rate for diffusion models
  weight_decay: 1.0e-6          # L2 regularization
  
  # Batch and memory settings - Optimized for quality over efficiency
  batch_size: 16               # Standard DDPM batch size for quality training
  num_workers: 8               # Conservative for stability (2 * num_gpus)
  accumulate_grad_batches: 4   # Effective batch size = 16 * 4 = 64 (quality focus)
  gradient_clip: 1.0           # Standard gradient clipping
  
  # Memory optimization - Quality-focused settings
  persistent_workers: true     # Keep workers alive between epochs
  prefetch_factor: 4           # Conservative prefetch for stability
  pin_memory: true             # Pin memory for faster GPU transfer
  drop_last: false             # Use all data for thorough training
  
  # Multi-GPU scaling (future-ready configuration)
  # Single GPU: batch_size: 16, accumulate_grad_batches: 4 (effective: 64)
  # 2x GPU: batch_size: 16, accumulate_grad_batches: 2 (effective: 64)
  # 4x GPU: batch_size: 16, accumulate_grad_batches: 1 (effective: 64)
  
  # Model training enhancements
  use_ema: true                 # Exponential moving average for stable inference
  use_conditioning: false       # Disable WF conditioning for pure self-supervised learning
  conditioning_type: none       # No conditioning for grayscale microscopy
  
  # Advanced learning rate scheduling
  use_scheduler: true           # Enable adaptive learning rate
  scheduler_type: cosine_with_restarts  # Advanced cosine annealing with warm restarts
  scheduler_warmup_steps: 1000  # Warmup steps for stable training
  scheduler_T_max: 50          # Cosine annealing period (epochs)
  scheduler_T_mult: 2          # Restart period multiplier
  scheduler_eta_min: 1e-7      # Minimum learning rate
  
  # Standard DDPM early stopping configuration
  early_stopping_patience: 50  # Epochs without improvement (standard DDPM patience)
  early_stopping_min_delta: 1e-5  # Minimum change to qualify as improvement
  early_stopping_monitor: val_loss    # Metric to monitor
  
  # Advanced scheduling options
  noise_schedule_type: cosine   # cosine, linear, sigmoid
  beta_start: 0.0001           # Standard DDPM noise schedule start
  beta_end: 0.02               # Standard DDPM noise schedule end
  use_karras_sigmas: true      # Use Karras noise schedule for better quality
  
  # Self-supervised learning specific parameters
  supervised_x0_weight: 0.0     # Disabled for pure self-supervised learning
  ddpm_loss_weight: 1.0         # Weight for standard DDPM loss
  cycle_loss_weight: 0.15       # Weight for cycle consistency loss (slightly higher)
  perceptual_loss_weight: 0.01  # Weight for perceptual loss
  cycle_loss_type: smooth_l1    # smooth_l1, l1, or l2 (smooth_l1 is more robust)
  use_perceptual_loss: true     # Enable perceptual loss for better quality
  
  # Standard DDPM validation and checkpointing (epoch-based)
  val_check_interval: 1.0      # Validate every epoch (standard DDPM)
  save_every_n_epochs: 5       # Save checkpoints every 5 epochs
  log_every_n_steps: 100       # Log metrics every 100 steps
  
  # Advanced training options with multi-resolution support
  steps_per_epoch: 563         # Corrected: 9000 training samples รท 16 batch_size = 563
  precision: 32-true           # Full precision for maximum quality (can use 16-mixed for speed)
  
  # Multi-resolution training enhancements
  resolution_curriculum:
    enabled: true              # Enable curriculum learning across resolutions
    difficulty_schedule: "progressive" # progressive, random, adaptive
    
    # Training phases
    phases:
      - resolution: 64
        epochs: 30
        focus: "global_structure"    # Learn global structure first
        augmentation_strength: 0.8   # Strong augmentation for robustness
        
      - resolution: 128  
        epochs: 50
        focus: "medium_details"      # Learn medium-scale features
        augmentation_strength: 0.6   # Moderate augmentation
        
      - resolution: 256
        epochs: 920                  # Remaining epochs for fine details
        focus: "fine_details"        # Learn fine-scale features
        augmentation_strength: 0.4   # Gentler augmentation
  
  # Standard DDPM validation configuration
  limit_val_batches: 1.0       # Use all validation data
  
  # Quality-focused training enhancements
  use_gradient_checkpointing: true  # Memory efficiency without sacrificing quality
  compile_model: false         # Disable for debugging, enable for production
  find_unused_parameters: false # For DDP stability
  
  # Multi-resolution specific optimizations
  adaptive_loss_scaling: true   # Scale loss based on resolution
  cross_resolution_consistency: true # Ensure consistency across scales
  frequency_domain_loss: 0.1    # Additional frequency-domain loss weight

# Data configuration - Standard DDPM with multi-resolution support
data:
  image_size: 256               # Standard DDPM resolution (upgradeable)
  min_intensity: 0              # Minimum intensity value
  max_intensity: 255            # Maximum intensity value
  noise_model: poisson          # poisson, gaussian, poisson_gaussian
  use_zarr: false               # Set to true for very large datasets
  use_self_supervised: false    # Use paired data for training
  
  # Advanced Multi-resolution Data Pipeline
  multi_resolution:
    enabled: true
    
    # Data preparation for multiple resolutions
    precompute_scales: true         # Pre-compute all resolution versions
    interpolation_method: "lanczos" # lanczos, bicubic, bilinear
    preserve_aspect_ratio: true     # Maintain aspect ratio during scaling
    
    # Resolution-specific augmentations
    scale_specific_augmentation:
      enabled: true
      low_res_augmentations:        # Stronger augmentation for low-res
        noise_std: 0.1
        blur_probability: 0.3
      high_res_augmentations:       # Gentler augmentation for high-res
        noise_std: 0.05
        blur_probability: 0.1
    
    # Multi-scale validation
    validate_all_scales: true       # Validate at all resolution scales
    
    # Progressive data complexity
    curriculum_learning:
      enabled: true
      start_with_simple: true       # Start with simpler/cleaner samples
      complexity_schedule: "linear" # linear, exponential, step
  
  # Generalized Anscombe Transform parameters (for poisson_gaussian noise)
  gat:
    alpha: 1.0                  # Poisson scaling factor
    mu: 0.0                     # Gaussian mean
    sigma: 0.0                  # Gaussian standard deviation

# PSF configuration for self-supervised cycle consistency
psf:
  type: gaussian                # Type of PSF (gaussian, file, measured)
  sigma_x: 2.0                  # Standard deviation in x direction (adjust for your system)
  sigma_y: 2.0                  # Standard deviation in y direction
  size: 21                      # PSF kernel size (should be odd)
  background: 0.0               # Background intensity level

# Physics configuration
physics:
  psf_type: gaussian
  psf_size: 21
  background: 0.0
  noise_type: poisson

# Guidance configuration
guidance:
  type: pkl                     # pkl, l2, anscombe
  epsilon: 1e-6                 # Guidance strength
  schedule_type: adaptive       # adaptive, linear, constant
  lambda_base: 0.1              # Base guidance weight
  schedule:
    T_threshold: 800            # Threshold for adaptive scheduling
    epsilon_lambda: 1e-3        # Adaptive scheduling parameter

# Evaluation configuration - Multi-resolution aware
evaluation:
  # Standard metrics computed at all resolutions
  metrics: ["psnr", "ssim", "frc", "lpips", "fid"]  # Comprehensive metric suite
  
  # Multi-resolution evaluation
  multi_scale_evaluation:
    enabled: true
    evaluate_at_scales: [128, 256] # Evaluate at multiple resolutions
    cross_scale_consistency: true   # Check consistency across scales
    frequency_domain_metrics: true # FFT-based quality metrics
  
  # Sample generation
  save_samples: true            # Save sample outputs during training
  num_samples: 16               # More samples for better evaluation
  samples_per_resolution: 4     # Samples at each resolution
  eval_every_n_epochs: 3        # More frequent evaluation
  
  # Progressive evaluation strategy
  resolution_specific_metrics:
    low_res: ["psnr", "ssim"]    # Focus on basic quality at low res
    high_res: ["lpips", "fid", "frc"] # Focus on perceptual quality at high res
  
  # Quality benchmarking
  benchmark_against_baselines: true  # Compare with bicubic upsampling, etc.

# Inference configuration - Multi-resolution sampling
inference:
  # Standard sampling parameters
  ddim_steps: 50                # Number of DDIM sampling steps
  eta: 0.0                      # DDIM eta parameter (0.0 = deterministic)
  use_autocast: true            # Use automatic mixed precision
  batch_size: 4                 # Inference batch size
  guidance_scale: 1.0           # Guidance scale for sampling
  
  # Multi-resolution inference
  multi_resolution_sampling:
    enabled: true
    strategy: "cascaded"          # cascaded, direct, hybrid
    
    # Cascaded super-resolution sampling
    cascaded_sampling:
      start_resolution: 64        # Start sampling at low resolution
      intermediate_steps: [20, 30, 50] # DDIM steps at each resolution
      upsampling_method: "learned" # learned, bicubic, lanczos
      noise_injection: 0.1        # Add noise when upsampling
    
    # Quality optimization
    self_attention_guidance: true  # Use self-attention for better details
    frequency_domain_loss: true    # Additional frequency-domain guidance
  
  # Advanced sampling techniques
  progressive_distillation: false # Enable for faster sampling (experimental)
  classifier_free_guidance: false # Enable for conditional generation

# Hardware optimization - Scalable GPU setup
hardware:
  devices: 1                    # Number of GPUs (1x A40, scalable to multi-GPU)
  accelerator: gpu              # gpu, cpu, tpu
  strategy: auto                # auto for single GPU, ddp for multi-GPU
  sync_batchnorm: false         # Will be auto-enabled for multi-GPU
  
  # Multi-GPU preparation (for HPC deployment)
  multi_gpu:
    enabled: false              # Set to true when scaling to multiple GPUs
    strategy: ddp               # Distributed Data Parallel
    find_unused_parameters: false  # For gradient synchronization
    
  # Performance optimization
  benchmark_cudnn: true         # Enable cuDNN benchmarking for consistent input sizes
  deterministic: false          # Set to true for reproducible results (slower)
  float32_matmul_precision: medium  # medium, high, highest (quality vs speed trade-off)
  
# Logging and monitoring
logging:
  log_level: INFO               # DEBUG, INFO, WARNING, ERROR
  log_every_n_steps: 50         # More frequent logging for single GPU
  save_top_k: 3                 # Keep top K checkpoints
  monitor_metric: val_loss      # Metric to monitor for best checkpoint
  
# Memory and performance optimization - Quality-focused
optimization:
  compile_model: false          # PyTorch 2.0 model compilation (enable for production)
  channels_last: false          # Use channels last memory format (can improve performance)
  benchmark_cudnn: true         # Enable cuDNN benchmarking
  deterministic: false          # Deterministic training (slower but reproducible)
  
  # Advanced optimization
  use_gradient_checkpointing: true   # Trade compute for memory
  max_split_size_mb: 512        # Control memory fragmentation
  empty_cache_steps: 100        # Clear cache every N steps
  
  # Quality vs efficiency balance
  priority: quality             # quality, balanced, efficiency
  
  # Automatic mixed precision (AMP) settings
  amp:
    enabled: false             # Set to true for 16-bit training (2x speed, slight quality loss)
    opt_level: O1              # O0 (FP32), O1 (conservative), O2 (aggressive)
    loss_scale: dynamic        # dynamic, static, or float value
  
# Data augmentation - Standard DDPM augmentations
augmentation:
  enabled: true                 # Enable standard DDPM augmentations
  
  # Geometric augmentations (standard for DDPM)
  horizontal_flip: 0.5          # Probability of horizontal flip
  vertical_flip: 0.5            # Probability of vertical flip
  rotation: 0.1                 # Maximum rotation angle (fraction of 2ฯ)
  
  # Advanced augmentations for microscopy
  random_crop: true             # Random cropping with scale
  crop_scale: [0.8, 1.0]        # Scale range for random crop
  elastic_transform: true       # Elastic deformation (good for biological images)
  elastic_alpha: 50.0           # Elastic deformation strength
  elastic_sigma: 5.0            # Elastic deformation smoothness
  
  # Intensity augmentations (preserve microscopy characteristics)
  brightness: 0.1               # Brightness variation
  contrast: 0.1                 # Contrast variation
  gamma: [0.8, 1.2]             # Gamma correction range
  
  # Noise augmentations (domain-specific)
  add_noise: true               # Add training noise
  noise_std: 0.05               # Standard deviation of added noise
  
  # Quality control
  augment_probability: 0.8      # Probability of applying any augmentation
  
# Resume training (optional)
# resume_checkpoint: null       # Path to checkpoint to resume from
# resume_start_epoch: 0         # Epoch to start from when resuming
