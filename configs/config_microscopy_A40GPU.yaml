defaults:
  - model: unet
  - data: real_pairs
  - physics: microscopy
  - guidance: pkl
  - training: ddpm
  - override hydra/launcher: basic
  - _self_

# Experiment configuration
experiment:
  name: microscopy_A40GPU_${now:%Y-%m-%d_%H-%M-%S}
  seed: 42
  device: cuda  # auto, cuda, cpu
  mixed_precision: true  # Use mixed precision for better memory efficiency

# Path configuration - Local server paths
paths:
  root: ${oc.env:PROJECT_ROOT,.}
  data: ${paths.root}/data
  checkpoints: ${paths.root}/checkpoints
  outputs: ${paths.root}/outputs
  logs: ${paths.root}/logs

# Weights & Biases logging
wandb:
  project: pkl-diffusion-microscopy-A40
  entity: anna_yoon-uc-berkeley  # Auto-configured from system credentials
  mode: offline  # Default to offline for local server

# Model configuration - Standard DDPM architecture for high-quality results
model:
  sample_size: 256                # Standard DDPM resolution (upgradeable to 512)
  in_channels: 1                   # Will be set to 2 automatically if use_conditioning=true
  out_channels: 1
  layers_per_block: 2
  block_out_channels: [64, 128, 256, 512]  # Standard DDPM channel progression
  down_block_types: ["DownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D"]  # More attention layers
  up_block_types: ["AttnUpBlock2D", "AttnUpBlock2D", "AttnUpBlock2D", "UpBlock2D"]            # More attention layers
  class_embed_type: null           # Can be enabled for conditional generation
  cross_attention_dim: null        # Can be enabled for text/class conditioning
  # Advanced Multi-Resolution Training (SOTA Implementation Available)
  # Your codebase has full ProgressiveTrainer implementation
  multi_resolution:
    enabled: true                   # Enable advanced progressive training
    strategy: "hierarchical"        # hierarchical, progressive, cascaded
    
    # Resolution curriculum (implemented in ProgressiveTrainer)
    # PROGRESSIVE STRATEGY: Start small, work up - much more data efficient!
    resolutions: [32, 64, 96, 128]  # Progressive resolution schedule [32, 64, 96, 128]
    base_resolution: 32             # Starting resolution (architecturally sound)
    target_resolution: 128          # Final target resolution (achievable with 9K samples)
    
    # Advanced progressive features (all implemented)
    progressive:
      enabled: true                 # Use ProgressiveTrainer class
      max_resolution: 128           # Maximum resolution (achievable target)
      curriculum_type: "exponential" # linear, exponential, adaptive
      epochs_per_resolution: [30, 40, 50, 80]  # Progressive epochs: 32→64→96→128
      
      # Smooth transitions (implemented)
      smooth_transitions: true      # Alpha-blending between resolutions
      transition_epochs: 3          # Epochs for smooth transitions
      blend_mode: "alpha"           # alpha, feature, loss
      
      # Adaptive features (implemented)
      lr_scaling: true              # Scale learning rate with resolution
      lr_curriculum: "sqrt"         # sqrt, linear, constant
      batch_scaling: true           # Adaptive batch scaling
      adaptive_batch_scaling: true  # Memory-aware batch scaling

# Training configuration - Optimized for single A40 GPU (46GB VRAM)
training:
  # Core training parameters
  max_epochs: 1000              # Maximum training epochs (will stop early if converged)
  num_timesteps: 1000           # Diffusion timesteps (1000 is optimal for quality)
  beta_schedule: cosine         # cosine schedule works better than linear
  learning_rate: 1.0e-4         # Stable learning rate for diffusion models
  weight_decay: 1.0e-6          # L2 regularization
  
  # Batch and memory settings - Adaptive for multi-resolution training
  batch_size: 32               # Larger batches for small patches (16x16, 32x32), auto-scaled down for larger
  num_workers: 8               # Conservative for stability (2 * num_gpus)
  accumulate_grad_batches: 4   # Effective batch size = 32 * 4 = 128 (excellent for small patches)
  gradient_clip: 1.0           # Standard gradient clipping
  
  # Memory optimization - Quality-focused settings
  persistent_workers: true     # Keep workers alive between epochs
  prefetch_factor: 4           # Conservative prefetch for stability
  pin_memory: true             # Pin memory for faster GPU transfer
  drop_last: false             # Use all data for thorough training
  
  # Multi-GPU scaling (future-ready configuration)
  # Single GPU: batch_size: 16, accumulate_grad_batches: 4 (effective: 64)
  # 2x GPU: batch_size: 16, accumulate_grad_batches: 2 (effective: 64)
  # 4x GPU: batch_size: 16, accumulate_grad_batches: 1 (effective: 64)
  
  # Model training enhancements
  use_ema: true                 # Exponential moving average for stable inference
  use_conditioning: false       # Disable WF conditioning for pure self-supervised learning
  conditioning_type: none       # No conditioning for grayscale microscopy
  
  # Advanced Learning Rate Scheduling (SOTA Implementation)
  use_scheduler: true           # Enable advanced scheduling
  scheduler_type: improved_cosine # improved_cosine, exponential, polynomial, adaptive
  scheduler_warmup_steps: 1000  # Warmup steps for stable training
  
  # Advanced scheduler configuration (implemented in SchedulerManager)
  advanced_schedulers:
    enabled: true               # Use SchedulerManager for dynamic switching
    primary_scheduler: "improved_cosine"
    alternatives:               # Alternative schedulers for dynamic switching
      - name: "exponential"
        type: "exponential"
        params:
          decay_rate: 0.96
      - name: "adaptive"
        type: "adaptive"
        params:
          adaptation_rate: 0.01
  
  # Early stopping configuration
  early_stopping_patience: 50  # Epochs without improvement
  early_stopping_min_delta: 1e-5  # Minimum change to qualify as improvement
  early_stopping_monitor: val_loss    # Metric to monitor
  
  # Advanced noise scheduling (implemented)
  beta_schedule: improved_cosine # Use advanced cosine schedule
  noise_schedule_params:
    offset: 0.008               # Improved cosine offset
    power: 1.0                  # Schedule power
    use_karras_sigmas: true     # Karras noise schedule
  
  # Self-supervised learning specific parameters
  supervised_x0_weight: 0.0     # Disabled for pure self-supervised learning
  ddpm_loss_weight: 1.0         # Weight for standard DDPM loss
  cycle_loss_weight: 0.15       # Weight for cycle consistency loss (slightly higher)
  perceptual_loss_weight: 0.01  # Weight for perceptual loss
  cycle_loss_type: smooth_l1    # smooth_l1, l1, or l2 (smooth_l1 is more robust)
  use_perceptual_loss: true     # Enable perceptual loss for better quality
  
  # Standard DDPM validation and checkpointing (epoch-based)
  val_check_interval: 1.0      # Validate every epoch (standard DDPM)
  save_every_n_epochs: 5       # Save checkpoints every 5 epochs
  log_every_n_steps: 100       # Log metrics every 100 steps
  
  # Advanced training options with multi-resolution support
  steps_per_epoch: 1125        # Corrected: 9000 training samples ÷ 8 batch_size = 1125
  precision: 32-true           # Full precision for maximum quality (can use 16-mixed for speed)
  
  # Standard DDPM validation configuration
  limit_val_batches: 1.0       # Use all validation data
  
  # Advanced training enhancements (all implemented)
  use_gradient_checkpointing: true   # Memory optimization (implemented)
  compile_model: false         # Disable for debugging, enable for production
  
  # Training optimizations (implemented)
  use_ema: true                # Exponential moving average
  ema_decay: 0.9999            # EMA decay rate
  
  # Advanced loss configuration (implemented in CompositeLoss)
  advanced_losses:
    enabled: true              # Enable advanced loss functions
    
    # Frequency domain losses (implemented)
    frequency_losses:
      enabled: true            # Enable frequency-domain losses
      fourier_loss_weight: 0.1 # Fourier loss weight
      wavelet_loss_weight: 0.05 # Wavelet loss weight
      high_freq_loss_weight: 0.02 # High frequency preservation
    
    # Multi-scale consistency (implemented in ProgressiveTrainer)
    multi_scale_consistency:
      enabled: true            # Cross-resolution consistency
      consistency_weight: 0.15 # Consistency loss weight
      
    # Perceptual losses (implemented)
    perceptual_loss:
      enabled: true            # Enable perceptual loss
      network: "vgg16"         # vgg16, resnet50
      layers: ["relu2_2", "relu3_3", "relu4_3"]
      weight: 0.01

# Data configuration - Standard DDPM with multi-resolution support
data:
  image_size: 128               # Target resolution (achievable with 9K samples)
  min_intensity: 0              # Minimum intensity value
  max_intensity: 255            # Maximum intensity value
  noise_model: poisson          # poisson, gaussian, poisson_gaussian
  use_zarr: false               # Set to true for very large datasets
  use_self_supervised: false    # Use paired data for training
  
  # Advanced Multi-resolution Data Pipeline (implemented)
  # ProgressiveDataLoader handles resolution scheduling
  data_loading:
    # Standard data loading
    use_zarr: false               # Use standard image loading
    cache_dataset: true           # Enable dataset caching for efficiency
    
    # Multi-resolution data pipeline (implemented in ProgressiveDataLoader)
    multi_resolution:
      enabled: true               # Use ProgressiveDataLoader
      precompute_scales: true     # Pre-compute resolution versions
      interpolation_method: "lanczos"  # High-quality interpolation
      preserve_aspect_ratio: true # Maintain aspect ratios
      
      # Resolution-specific augmentations (implemented)
      scale_specific_augmentation:
        enabled: true
        low_res_augmentations:    # Stronger augmentation for low-res
          noise_std: 0.1
          blur_probability: 0.3
        high_res_augmentations:   # Gentler augmentation for high-res
          noise_std: 0.05
          blur_probability: 0.1
  
  # Generalized Anscombe Transform parameters (for poisson_gaussian noise)
  gat:
    alpha: 1.0                  # Poisson scaling factor
    mu: 0.0                     # Gaussian mean
    sigma: 0.0                  # Gaussian standard deviation

# PSF configuration for self-supervised cycle consistency
psf:
  type: gaussian                # Type of PSF (gaussian, file, measured)
  sigma_x: 2.0                  # Standard deviation in x direction (adjust for your system)
  sigma_y: 2.0                  # Standard deviation in y direction
  size: 21                      # PSF kernel size (should be odd)
  background: 0.0               # Background intensity level

# Physics configuration
physics:
  psf_type: gaussian
  psf_size: 21
  background: 0.0
  noise_type: poisson

# Guidance configuration
guidance:
  type: pkl                     # pkl, l2, anscombe
  epsilon: 1e-6                 # Guidance strength
  schedule_type: adaptive       # adaptive, linear, constant
  lambda_base: 0.1              # Base guidance weight
  schedule:
    T_threshold: 800            # Threshold for adaptive scheduling
    epsilon_lambda: 1e-3        # Adaptive scheduling parameter

# Advanced Evaluation Configuration (comprehensive implementation)
evaluation:
  # Comprehensive metrics suite (implemented in EvaluationSuite)
  metrics: ["psnr", "ssim", "frc", "lpips", "fid"]  # Full metric suite
  
  # Multi-resolution evaluation (implemented)
  multi_scale_evaluation:
    enabled: true               # Evaluate at all training resolutions
    evaluate_at_scales: [32, 64, 96, 128] # Match training resolutions [32, 64, 96, 128]
    cross_scale_consistency: true # Check consistency across scales
    frequency_domain_metrics: true # FFT-based quality metrics
  
  # Sample generation
  save_samples: true            # Save sample outputs during training
  num_samples: 16               # More samples for comprehensive evaluation
  samples_per_resolution: 4     # Samples at each resolution
  eval_every_n_epochs: 3        # More frequent evaluation
  
  # Advanced evaluation features (implemented)
  robustness_testing:
    enabled: true               # Test model robustness
    noise_levels: [0.1, 0.2, 0.3] # Test different noise levels
    
  # Downstream task evaluation (implemented for microscopy)
  downstream_tasks:
    enabled: true               # Evaluate on downstream tasks
    tasks: ["segmentation", "detection"] # Microscopy-specific tasks

# Advanced Inference Configuration (comprehensive sampling suite)
inference:
  # Standard sampling parameters
  ddim_steps: 50                # Number of DDIM sampling steps
  eta: 0.0                      # DDIM eta parameter (0.0 = deterministic)
  use_autocast: true            # Use automatic mixed precision
  batch_size: 4                 # Inference batch size
  guidance_scale: 1.0           # Guidance scale for sampling
  
  # Advanced sampling options (implemented)
  clip_denoised: true           # Clip predicted x0 to [-1,1]
  v_parameterization: false     # Use epsilon parameterization
  
  # Multi-resolution sampling (implemented in CascadedSampler)
  cascaded_sampling:
    enabled: true               # Use cascaded super-resolution sampling
    start_resolution: 64        # Start sampling at low resolution
    intermediate_steps: [20, 30, 50] # DDIM steps at each resolution
    upsampling_method: "learned" # learned, bicubic, lanczos
    noise_injection: 0.1        # Add noise when upsampling
  
  # Progressive sampling (implemented in ProgressiveSampler)
  progressive_sampling:
    enabled: true               # Use progressive sampling
    strategy: "hierarchical"    # hierarchical, sequential
    
  # Advanced sampling techniques (implemented)
  sampler_options:
    ancestral_sampling: false   # Use AncestralSampler
    dpm_solver: false          # Use DPM-Solver++ (if available)
    adaptive_steps: true        # Adaptive step sizing

# Hardware optimization - Scalable GPU setup
hardware:
  devices: 1                    # Number of GPUs (1x A40, scalable to multi-GPU)
  accelerator: gpu              # gpu, cpu, tpu
  strategy: auto                # auto for single GPU, ddp for multi-GPU
  sync_batchnorm: false         # Will be auto-enabled for multi-GPU
  
  # Multi-GPU preparation (for HPC deployment)
  multi_gpu:
    enabled: false              # Set to true when scaling to multiple GPUs
    strategy: ddp               # Distributed Data Parallel
    find_unused_parameters: false  # For gradient synchronization
    
  # Performance optimization
  benchmark_cudnn: true         # Enable cuDNN benchmarking for consistent input sizes
  deterministic: false          # Set to true for reproducible results (slower)
  float32_matmul_precision: medium  # medium, high, highest (quality vs speed trade-off)
  
# Logging and monitoring
logging:
  log_level: INFO               # DEBUG, INFO, WARNING, ERROR
  log_every_n_steps: 50         # More frequent logging for single GPU
  save_top_k: 3                 # Keep top K checkpoints
  monitor_metric: val_loss      # Metric to monitor for best checkpoint
  
# Advanced Memory and Performance Optimization (SOTA implementation)
optimization:
  compile_model: false          # PyTorch 2.0 model compilation (enable for production)
  channels_last: true           # Use channels last memory format (better performance)
  benchmark_cudnn: true         # Enable cuDNN benchmarking
  deterministic: false          # Deterministic training (slower but reproducible)
  
  # Advanced memory optimization (implemented in AdaptiveBatchSizer)
  memory_optimization:
    enabled: true               # Enable adaptive memory management
    use_gradient_checkpointing: true # Trade compute for memory
    adaptive_batch_sizing: true # Automatically adjust batch size
    safety_factor: 0.85         # Memory safety factor
    memory_pressure_threshold: 0.9 # Threshold for batch size reduction
    max_split_size_mb: 512      # Control memory fragmentation
    empty_cache_steps: 100      # Clear cache every N steps
  
  # Performance optimization
  performance:
    priority: balanced          # quality, balanced, efficiency
    enable_profiling: false     # Enable performance profiling
    
  # Automatic mixed precision (AMP) settings
  amp:
    enabled: true              # Enable 16-bit training (2x speed, minimal quality loss)
    opt_level: O1              # O0 (FP32), O1 (conservative), O2 (aggressive)
    loss_scale: dynamic        # dynamic, static, or float value
    
  # Advanced features (implemented)
  advanced_features:
    enable_frequency_losses: true    # Enable frequency domain losses
    enable_advanced_schedulers: true # Enable advanced scheduling
    enable_memory_optimization: true # Enable memory optimization
    enable_cascaded_sampling: true   # Enable cascaded sampling
  
# Advanced Data Augmentation - Research-Grade
augmentation:
  enabled: true                 # Enable comprehensive augmentations
  
  # Geometric augmentations (research-standard)
  horizontal_flip: 0.5          # Probability of horizontal flip
  vertical_flip: 0.5            # Probability of vertical flip
  rotation: 0.15                # Maximum rotation angle (increased for robustness)
  
  # Advanced geometric augmentations for microscopy
  random_crop: true             # Random cropping with scale
  crop_scale: [0.85, 1.0]       # Scale range for random crop (more conservative)
  elastic_transform: true       # Elastic deformation (excellent for biological images)
  elastic_alpha: 40.0           # Elastic deformation strength (tuned for microscopy)
  elastic_sigma: 4.0            # Elastic deformation smoothness
  
  # Intensity augmentations (preserve microscopy characteristics)
  brightness: 0.15              # Brightness variation (increased)
  contrast: 0.15                # Contrast variation (increased)
  gamma: [0.7, 1.3]             # Gamma correction range (wider range)
  
  # Advanced noise augmentations (domain-specific)
  add_noise: true               # Add training noise
  noise_std: 0.08               # Standard deviation of added noise (increased)
  poisson_noise: true           # Add Poisson noise (microscopy-specific)
  gaussian_blur: 0.2            # Probability of Gaussian blur
  
  # Resolution-aware augmentation (implemented in ProgressiveTrainer)
  resolution_aware: true        # Different augmentation per resolution
  scale_augmentation_strength: true # Stronger augmentation at lower resolutions
  
  # Quality control and efficiency
  augment_probability: 0.85     # High probability for robust training
  fast_augmentation: true       # Use optimized augmentation pipeline
  
# Advanced Training Configuration
# To use these advanced features, modify run_microscopy.py to use:
# from pkl_dg.models import ProgressiveTrainer, create_progressive_config
# trainer = ProgressiveTrainer(model=unet, config=training_cfg, ...)

# Resume training (optional)
# resume_checkpoint: null       # Path to checkpoint to resume from
# resume_start_epoch: 0         # Epoch to start from when resuming

# Performance Notes:
# - This configuration uses state-of-the-art features from your codebase
# - Memory usage optimized with adaptive batch sizing
# - Progressive training provides better convergence
# - Advanced schedulers improve sample quality
# - Comprehensive evaluation suite for research publication
