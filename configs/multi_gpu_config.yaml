defaults:
  - model: unet
  - data: real_pairs
  - physics: microscopy
  - guidance: pkl
  - training: ddpm
  - override hydra/launcher: basic
  - _self_

# MULTI-GPU CONFIGURATION FOR MAXIMUM SPEED - 4x A40 GPUs
# Optimized for 4x A40 GPUs on Savio HPC (1M steps, dual objectives)

# Experiment configuration
experiment:
  name: microscopy_4xA40_1M_steps_${now:%Y-%m-%d_%H-%M-%S}
  seed: 42
  device: cuda
  mixed_precision: true

# Path configuration - HPC optimized paths
paths:
  code_root: /global/home/users/anna_yoon/PKL-DiffusionDenoising
  data_root: /global/scratch/users/anna_yoon
  data: ${paths.data_root}/data/real_microscopy  # Access to 128x128 patch dataset
  checkpoints: ${paths.data_root}/checkpoints
  outputs: ${paths.data_root}/outputs
  logs: ${paths.data_root}/logs

# Weights & Biases logging
wandb:
  project: pkl-diffusion-microscopy
  entity: anna_yoon-uc-berkeley
  mode: offline  # Default to offline for HPC clusters

# Model configuration - Multi-resolution with 4x A40 GPU support
model:
  sample_size: 128                  # Target resolution for progressive training
  in_channels: 1
  out_channels: 1
  layers_per_block: 2
  block_out_channels: [96, 192, 384, 384]  # Optimized architecture (same as single GPU config)
  down_block_types: ["DownBlock2D", "DownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D"]
  up_block_types: ["AttnUpBlock2D", "AttnUpBlock2D", "UpBlock2D", "UpBlock2D"]
  class_embed_type: null
  cross_attention_dim: null
  learned_variance: true
  
  # Multi-Resolution Training Configuration
  multi_resolution:
    enabled: true                   # Enable progressive training
    strategy: "hierarchical"        # hierarchical, progressive, cascaded
    
    # Resolution curriculum - Progressive resolution schedule
    resolutions: [32, 64, 96, 128]  # Progressive resolution schedule [32, 64, 96, 128]
    base_resolution: 32             # Starting resolution
    target_resolution: 128          # Final target resolution
    
    # Progressive training features
    progressive:
      enabled: true                 # Use ProgressiveTrainer class
      max_resolution: 128           # Maximum resolution
      curriculum_type: "exponential" # linear, exponential, adaptive
      # Explicit resolution schedule passed to trainer
      resolution_schedule: [32, 64, 96, 128]
      steps_per_resolution: [200000, 250000, 300000, 250000]  # Progressive steps: 32→64→96→128 (1M total)
      
      # Smooth transitions
      smooth_transitions: true      # Alpha-blending between resolutions
      transition_steps: 5000        # 5K steps for smooth transitions between resolutions
      blend_mode: "alpha"           # alpha, feature, loss
      
      # Adaptive features
      lr_scaling: true              # Scale learning rate with resolution
      lr_curriculum: "sqrt"         # sqrt, linear, constant
      batch_scaling: true           # Adaptive batch scaling
      adaptive_batch_scaling: true  # Memory-aware batch scaling

# Multi-GPU Training configuration - OPTIMIZED for 4x A40 GPUs (1M steps)
training:
  # Core training parameters - STEP-BASED TRAINING
  max_steps: 1000000                # 1 Million steps (much better for diffusion models)
  max_epochs: -1                    # Disable epoch limit (step-based training)
  num_timesteps: 1000               # Full timesteps for quality
  beta_schedule: cosine
  learning_rate: 2.0e-4             # Higher LR for larger effective batch (4x GPUs)
  weight_decay: 1.0e-6
  
  # Multi-GPU batch settings - MAXIMIZED for 4x A40 GPUs
  batch_size: 32                    # Per GPU batch size (total = 32*4 = 128)
  num_workers: 8                    # Per GPU workers (total = 8*4 = 32 workers)
  accumulate_grad_batches: 1        # No accumulation needed (effective batch = 128)
  gradient_clip: 1.0
  
  # Memory optimization for 4x A40 GPUs
  persistent_workers: true
  prefetch_factor: 8                # Higher prefetch for multi-GPU
  pin_memory: true
  drop_last: false                  # Use all data samples
  
  # Model training enhancements
  use_ema: true
  use_conditioning: false
  conditioning_type: none
  
  # ELBO (variational bound) hybrid objective (Nichol & Dhariwal 2021)
  use_elbo_loss: true
  combine_elbo_with_simple: true
  elbo_weight: 1.0
  elbo_sigma0: 1.0
  elbo_observation: poisson
  elbo_poisson_epsilon: 1e-6
  learned_variance: true
  min_logvar: -10.0
  max_logvar: 2.0
  
  # ADAPTIVE LEARNING RATE SCHEDULING - Optimized for progressive training
  use_scheduler: true
  scheduler_type: "improved_cosine"  # Advanced cosine with restarts
  scheduler_warmup_steps: 1000
  
  # Advanced scheduler configuration
  advanced_schedulers:
    enabled: true               # Use SchedulerManager for dynamic switching
    primary_scheduler: "improved_cosine"
    scheduler_params:
      T_max: 50000             # Cosine period (50K steps) - matches resolution phases
      eta_min: 1e-6            # Minimum learning rate
      T_mult: 2                # Period multiplier for restarts
      restart_decay: 0.8       # Decay factor after each restart
  
  # EARLY STOPPING - Step-based for 1M step training
  early_stopping_patience: 50000  # 50K steps patience (equivalent to ~190 epochs)
  early_stopping_min_delta: 1e-6  # Sensitive to small improvements
  early_stopping_monitor: val_loss
  early_stopping_mode: "min"
  early_stopping_verbose: true
  
  # DUAL OBJECTIVE TRAINING - disabled to focus on hybrid DDPM objective and inference-time guidance
  use_dual_objective_loss: false
  
  # Legacy parameters (kept for compatibility)
  supervised_x0_weight: 0.0
  ddpm_loss_weight: 1.0
  cycle_loss_weight: 0.15
  perceptual_loss_weight: 0.0   # DISABLED - Pure self-supervised
  cycle_loss_type: smooth_l1
  use_perceptual_loss: false    # DISABLED - Pure self-supervised

  # Light physics forward-consistency (keeps unconditional prior; improves generalization)
  use_forward_consistency: true         # Enable small forward-consistency loss
  forward_consistency_weight: 0.01      # Small lambda for stability
  forward_consistency_type: l2          # l2 | kl | pkl
  forward_consistency_sigma2: 1.0       # For kl type
  forward_consistency_epsilon: 1e-6     # For pkl type numerical stability
  forward_consistency_warmup_steps: 2000
  
  # STEP-BASED validation and checkpointing
  val_check_interval: 500       # Validate every 500 steps (must be ≤ batches per epoch)
  save_every_n_steps: 10000     # Save checkpoint every 10000 steps
  log_every_n_steps: 100        # Log metrics every 100 steps
  
  # COMPREHENSIVE MODEL SAVING STRATEGY
  checkpoint_every_n_steps: [5000, 10000]  # Multiple checkpoint intervals
  save_intermediate_checkpoints: true      # Save at both intervals
  
  # Advanced training options
  steps_per_epoch: 66           # UPDATED: 33,635 samples ÷ (32*4) = 263 steps/epoch for 4x GPUs
  precision: 16-mixed
  
  # PERFORMANCE OPTIMIZATIONS - Maximize 4x A40 utilization
  compile_model: true           # PyTorch 2.0 compilation for 15-20% speedup
  use_fused_adam: true          # Fused AdamW optimizer for speed
  dataloader_pin_memory_device: "cuda"  # Pin directly to GPU

# Data configuration - 128x128 patches with 16-bit support
data:
  image_size: 128                   # Target resolution for progressive training
  min_intensity: 0                  # Minimum intensity value (16-bit)
  max_intensity: 65535              # Maximum intensity value (16-bit range)
  noise_model: poisson              # Poisson noise model for microscopy
  use_16bit_normalization: true     # Proper 16-bit normalization
  align_pairs: true                 # Align WF to 2P images
  use_self_supervised: true         # Use self-supervised learning with forward model
  
  
  # Multi-resolution Data Pipeline
  data_loading:
    # Standard data loading
    
    cache_dataset: true           # Enable dataset caching for efficiency
    
    # Multi-resolution data pipeline
    multi_resolution:
      enabled: true               # Use ProgressiveDataLoader
      precompute_scales: true     # Pre-compute resolution versions
      interpolation_method: "lanczos"  # High-quality interpolation
      preserve_aspect_ratio: true # Maintain aspect ratios
      
      # Resolution-specific augmentations
      scale_specific_augmentation:
        enabled: true
        low_res_augmentations:    # Stronger augmentation for low-res
          noise_std: 0.1
          blur_probability: 0.3
        high_res_augmentations:   # Gentler augmentation for high-res
          noise_std: 0.05
          blur_probability: 0.1
  
  # Generalized Anscombe Transform parameters
  gat:
    alpha: 1.0
    mu: 0.0
    sigma: 0.0

# PSF configuration for self-supervised cycle consistency - 2μm bead
psf:
  type: gaussian                # Type of PSF (gaussian, file, measured)
  sigma_x: 2.0                  # Standard deviation in x direction (2μm bead)
  sigma_y: 2.0                  # Standard deviation in y direction (2μm bead)
  size: 21                      # PSF kernel size (should be odd)
  background: 0.0               # Background intensity level

# Physics configuration
physics:
  use_psf: true
  use_bead_psf: true
  beads_dir: ${paths.data}/beads  # Use 2μm bead PSF from data/real_microscopy/beads/
  bead_mode: with_AO  # Use the AO-corrected PSF
  psf_type: measured  # Use measured PSF from beads
  background: 0.0
  noise_type: poisson

# Guidance configuration
guidance:
  type: pkl
  epsilon: 1e-6
  schedule_type: adaptive
  lambda_base: 0.1
  schedule:
    T_threshold: 800
    epsilon_lambda: 1e-3

# Evaluation configuration
evaluation:
  metrics: ["psnr", "ssim", "frc"]  # Full metrics
  save_samples: true
  num_samples: 8
  eval_every_n_epochs: 5

# Inference configuration
inference:
  ddim_steps: 50                    # Full inference steps
  eta: 0.0
  use_autocast: true
  batch_size: 8                     # Per GPU inference batch
  guidance_scale: 1.0

# Hardware optimization for 4x A40 GPUs
hardware:
  devices: 4                        # 4x A40 GPUs
  accelerator: gpu
  strategy: ddp                     # Distributed Data Parallel
  sync_batchnorm: true              # Sync batch norm across GPUs
  
# Logging and monitoring
logging:
  log_level: INFO
  log_every_n_steps: 100
  save_top_k: 3
  monitor_metric: val_loss
  
# Memory and performance optimization - MAXIMIZED for 4x A40
optimization:
  compile_model: true               # PyTorch 2.0 compilation (15-20% speedup)
  channels_last: true               # Use channels last memory format (better A40 performance)
  benchmark_cudnn: true             # Enable cuDNN benchmarking
  deterministic: false              # Non-deterministic for maximum speed
  
  # Advanced memory optimization for 4x A40
  memory_efficient_attention: true    # Use memory efficient attention
  gradient_checkpointing: true        # ENABLED: Trade compute for memory (allows larger batches)
  use_gradient_checkpointing: true    # Enable gradient checkpointing in training
  use_tf32: true                      # Enable TF32 on A40 for speed
  torch_compile_mode: "max-autotune"  # Maximum optimization
  
# MICROSCOPY-SPECIFIC DATA AUGMENTATION - Optimized for dual objectives
augmentation:
  enabled: true                 # ENABLED: Essential for robust microscopy training
  
  # Geometric augmentations (microscopy-safe)
  horizontal_flip: 0.5          # Safe for microscopy (no directional bias)
  vertical_flip: 0.5            # Safe for microscopy
  rotation: 0.1                 # Small rotations (preserve microscopy structures)
  
  # Advanced microscopy augmentations
  random_crop: true             # Random cropping with scale
  crop_scale: [0.9, 1.0]        # Conservative cropping (preserve structures)
  elastic_transform: true       # Excellent for biological images
  elastic_alpha: 20.0           # Moderate deformation (preserve biology)
  elastic_sigma: 3.0            # Smooth deformation
  
  # INTENSITY AUGMENTATIONS - Critical for dual objective training
  intensity_augmentation:
    enabled: true               # Essential for intensity mapping
    brightness: 0.05            # Subtle brightness changes (preserve signal)
    contrast: 0.05              # Subtle contrast changes
    gamma: [0.95, 1.05]         # Small gamma corrections
    
    # Microscopy-specific intensity augmentation
    intensity_scale_range: [0.98, 1.02]  # Preserve signal relationships
    noise_injection: true       # Add small noise to increase variation
    noise_std: 0.01             # Small noise (increase 2P variation)
    poisson_noise: true         # Microscopy-appropriate noise model
  
  # Quality control
  augment_probability: 0.8      # High probability for robust training
  preserve_intensity_mapping: true  # Critical for dual objectives
