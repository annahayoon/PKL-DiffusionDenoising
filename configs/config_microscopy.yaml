defaults:
  - model: unet
  - data: real_pairs
  - physics: microscopy
  - guidance: pkl
  - training: ddpm
  - override hydra/launcher: basic
  - _self_

# Experiment configuration
experiment:
  name: microscopy_self_supervised_${now:%Y-%m-%d_%H-%M-%S}
  seed: 42
  device: cuda  # auto, cuda, cpu
  mixed_precision: true  # Use mixed precision for better memory efficiency

# Path configuration
paths:
  root: ${oc.env:PROJECT_ROOT,.}
  data: ${paths.root}/data/real_microscopy  # Access to 128x128 patch dataset
  checkpoints: ${paths.root}/checkpoints
  outputs: ${paths.root}/outputs
  logs: ${paths.root}/logs

# Weights & Biases logging
wandb:
  project: pkl-diffusion-microscopy
  entity: anna_yoon-uc-berkeley  # Auto-configured from system credentials
  mode: offline  # Default to offline for HPC clusters

# Model configuration - Multi-resolution training with progressive schedule [16, 32, 64, 128]
model:
  sample_size: 128
  in_channels: 1  # Will be set to 2 automatically if use_conditioning=true
  out_channels: 1
  layers_per_block: 2
  block_out_channels: [96, 192, 384, 384]   # Same as A40GPU config (proven to work)
  down_block_types: ["DownBlock2D", "DownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D"]
  up_block_types: ["AttnUpBlock2D", "AttnUpBlock2D", "UpBlock2D", "UpBlock2D"]
  class_embed_type: null
  cross_attention_dim: null
  
  # Multi-Resolution Training Configuration
  multi_resolution:
    enabled: true                   # Enable progressive training
    strategy: "hierarchical"        # hierarchical, progressive, cascaded
    
    # Resolution curriculum - Progressive resolution schedule
    resolutions: [32, 64, 96, 128]  # Progressive resolution schedule [32, 64, 96, 128]
    base_resolution: 32             # Starting resolution
    target_resolution: 128          # Final target resolution
    
    # Progressive training features
    progressive:
      enabled: true                 # Use ProgressiveTrainer class
      max_resolution: 128           # Maximum resolution
      curriculum_type: "exponential" # linear, exponential, adaptive
      epochs_per_resolution: [30, 40, 50, 80]  # Progressive epochs: 32→64→96→128
      
      # Smooth transitions
      smooth_transitions: true      # Alpha-blending between resolutions
      transition_epochs: 3          # Epochs for smooth transitions
      blend_mode: "alpha"           # alpha, feature, loss
      
      # Adaptive features
      lr_scaling: true              # Scale learning rate with resolution
      lr_curriculum: "sqrt"         # sqrt, linear, constant
      batch_scaling: true           # Adaptive batch scaling
      adaptive_batch_scaling: true  # Memory-aware batch scaling

# Training configuration - Optimized for self-supervised microscopy learning
training:
  # Core training parameters
  max_epochs: 1000              # Maximum training epochs (will stop early if converged)
  num_timesteps: 1000           # Diffusion timesteps (1000 is optimal for quality)
  beta_schedule: cosine         # cosine schedule works better than linear
  learning_rate: 1.0e-4         # Stable learning rate for diffusion models
  weight_decay: 1.0e-6          # L2 regularization
  
  # Batch and memory settings - Optimized for single A40 GPU
  batch_size: 32                # Per GPU batch size (good for A40)
  num_workers: 16               # Workers for single GPU (adjust based on CPU cores)
  accumulate_grad_batches: 4    # Effective batch size = 32 * 4 = 128 (optimal for single A40)
  gradient_clip: 1.0            # Gradient clipping for stability
  
  # Memory optimization - Tuned for A40 (46GB VRAM)
  persistent_workers: true      # Keep workers alive between epochs
  prefetch_factor: 8            # Increased prefetch for high-core CPU
  pin_memory: true              # Pin memory for faster GPU transfer
  
  # GPU Memory scaling guide for 128x128 images (proven values):
  # - A40/A100 (40-80GB): batch_size: 32 + accumulate_grad_batches: 3 (current, proven)
  # - RTX 4090 (24GB): batch_size: 24 + accumulate_grad_batches: 3
  # - RTX 3090 (24GB): batch_size: 16 + accumulate_grad_batches: 4
  # - RTX 3080 (10GB): batch_size: 8 + accumulate_grad_batches: 4
  # - RTX 3070 (8GB): batch_size: 4 + accumulate_grad_batches: 6
  
  # Model training enhancements
  use_ema: true                 # Exponential moving average for stable inference
  use_conditioning: false       # Disable WF conditioning for pure self-supervised learning
  conditioning_type: none       # No conditioning for grayscale microscopy
  
  # Learning rate scheduling and early stopping
  use_scheduler: true           # Enable adaptive learning rate
  scheduler_type: cosine        # Cosine annealing scheduler
  scheduler_warmup_steps: 1000  # Warmup steps for learning rate
  
  # Early stopping configuration
  early_stopping_patience: 25  # Stop if no improvement for 25 epochs
  early_stopping_min_delta: 1e-5  # Minimum change to qualify as improvement
  early_stopping_monitor: val_loss  # Metric to monitor
  
  # Self-supervised learning specific parameters
  supervised_x0_weight: 0.0     # Disabled for pure self-supervised learning
  ddpm_loss_weight: 1.0         # Weight for standard DDPM loss
  cycle_loss_weight: 0.15       # Weight for cycle consistency loss (slightly higher)
  perceptual_loss_weight: 0.01  # Weight for perceptual loss
  cycle_loss_type: smooth_l1    # smooth_l1, l1, or l2 (smooth_l1 is more robust)
  use_perceptual_loss: true     # Enable perceptual loss for better quality
  
  # Training validation and checkpointing
  val_check_interval: 1.0       # Validate every epoch
  save_every_n_epochs: 10       # Save checkpoint every 10 epochs
  log_every_n_steps: 100        # Log metrics every 100 steps
  
  # Advanced training options
  steps_per_epoch: 263          # Corrected: 33,635 samples ÷ 128 effective batch size = 263 steps/epoch
  precision: 16-mixed           # Mixed precision training
  
# Data configuration - 128x128 patches with 16-bit support
data:
  image_size: 128               # 128x128 patches from frame-based splits
  min_intensity: 0              # Minimum intensity value (16-bit)
  max_intensity: 65535          # Maximum intensity value (16-bit range)
  noise_model: poisson          # Poisson noise model for microscopy
  use_16bit_normalization: true # Proper 16-bit normalization
  align_pairs: true             # Align WF to 2P images
  use_self_supervised: true     # Use self-supervised learning with forward model
  use_zarr: false               # Set to true for very large datasets
  
  # Multi-resolution Data Pipeline
  data_loading:
    # Standard data loading
    use_zarr: false               # Use standard image loading
    cache_dataset: true           # Enable dataset caching for efficiency
    
    # Multi-resolution data pipeline
    multi_resolution:
      enabled: true               # Use ProgressiveDataLoader
      precompute_scales: true     # Pre-compute resolution versions
      interpolation_method: "lanczos"  # High-quality interpolation
      preserve_aspect_ratio: true # Maintain aspect ratios
      
      # Resolution-specific augmentations
      scale_specific_augmentation:
        enabled: true
        low_res_augmentations:    # Stronger augmentation for low-res
          noise_std: 0.1
          blur_probability: 0.3
        high_res_augmentations:   # Gentler augmentation for high-res
          noise_std: 0.05
          blur_probability: 0.1
  
  # Generalized Anscombe Transform parameters (for poisson_gaussian noise)
  gat:
    alpha: 1.0                  # Poisson scaling factor
    mu: 0.0                     # Gaussian mean
    sigma: 0.0                  # Gaussian standard deviation

# PSF configuration for self-supervised cycle consistency - 2μm bead
psf:
  type: gaussian                # Type of PSF (gaussian, file, measured)
  sigma_x: 2.0                  # Standard deviation in x direction (2μm bead)
  sigma_y: 2.0                  # Standard deviation in y direction (2μm bead)
  size: 21                      # PSF kernel size (should be odd)
  background: 0.0               # Background intensity level

# Physics configuration
physics:
  use_psf: true
  use_bead_psf: true
  beads_dir: ${paths.data}/beads  # Use 2μm bead PSF from data/real_microscopy/beads/
  bead_mode: with_AO  # Use the AO-corrected PSF
  psf_type: measured  # Use measured PSF from beads
  background: 0.0
  noise_type: poisson

# Guidance configuration
guidance:
  type: pkl                     # pkl, l2, anscombe
  epsilon: 1e-6                 # Guidance strength
  schedule_type: adaptive       # adaptive, linear, constant
  lambda_base: 0.1              # Base guidance weight
  schedule:
    T_threshold: 800            # Threshold for adaptive scheduling
    epsilon_lambda: 1e-3        # Adaptive scheduling parameter

# Evaluation configuration
evaluation:
  metrics: ["psnr", "ssim", "frc"]  # Metrics to compute
  save_samples: true            # Save sample outputs during training
  num_samples: 8                # Number of samples to save
  eval_every_n_epochs: 5        # Evaluate every N epochs

# Inference configuration
inference:
  ddim_steps: 50                # Number of DDIM sampling steps
  eta: 0.0                      # DDIM eta parameter (0.0 = deterministic)
  use_autocast: true            # Use automatic mixed precision
  batch_size: 4                 # Inference batch size
  guidance_scale: 1.0           # Guidance scale for sampling

# Hardware optimization - Single A40 GPU setup
hardware:
  devices: 1                    # Single A40 GPU
  accelerator: gpu              # gpu, cpu, tpu
  strategy: auto                # Auto strategy for single GPU
  sync_batchnorm: false         # Not needed for single GPU
  
# Logging and monitoring
logging:
  log_level: INFO               # DEBUG, INFO, WARNING, ERROR
  log_every_n_steps: 100        # Log frequency
  save_top_k: 3                 # Keep top K checkpoints
  monitor_metric: val_loss      # Metric to monitor for best checkpoint
  
# Memory and performance optimization
optimization:
  compile_model: false          # PyTorch 2.0 model compilation (experimental)
  channels_last: false          # Use channels last memory format
  benchmark_cudnn: true         # Enable cuDNN benchmarking
  deterministic: false          # Deterministic training (slower but reproducible)
  
# Data augmentation (optional)
augmentation:
  enabled: false                # Enable data augmentation
  horizontal_flip: 0.5          # Probability of horizontal flip
  vertical_flip: 0.5            # Probability of vertical flip
  rotation: 0.1                 # Maximum rotation angle (fraction of 2π)
  
# Resume training (optional)
# resume_checkpoint: null       # Path to checkpoint to resume from
# resume_start_epoch: 0         # Epoch to start from when resuming
