defaults:
  - model: unet
  - data: real_pairs
  - physics: microscopy
  - guidance: pkl
  - training: ddpm
  - override hydra/launcher: basic
  - _self_

# Experiment configuration
experiment:
  name: microscopy_self_supervised_${now:%Y-%m-%d_%H-%M-%S}
  seed: 42
  device: cuda  # auto, cuda, cpu
  mixed_precision: true  # Use mixed precision for better memory efficiency

# Path configuration
paths:
  code_root: /home/jilab/anna_OS_ML/PKL-DiffusionDenoising
  data_root: /home/jilab/anna_OS_ML/PKL-DiffusionDenoising
  data: ${paths.data_root}/data/real_microscopy  # Access to 128x128 patch dataset
  checkpoints: ${paths.data_root}/checkpoints
  outputs: ${paths.data_root}/outputs
  logs: ${paths.data_root}/logs

# Weights & Biases logging
wandb:
  project: pkl-diffusion-microscopy
  entity: anna_yoon-uc-berkeley  # Auto-configured from system credentials
  mode: offline  # Default to offline for HPC clusters

# Model configuration - Multi-resolution training with progressive schedule [16, 32, 64, 128]
model:
  sample_size: 128
  in_channels: 1  # Will be set to 2 automatically if use_conditioning=true
  out_channels: 1
  layers_per_block: 2
  block_out_channels: [96, 192, 384, 384]   # Same as A40GPU config (proven to work)
  down_block_types: ["DownBlock2D", "DownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D"]
  up_block_types: ["AttnUpBlock2D", "AttnUpBlock2D", "UpBlock2D", "UpBlock2D"]
  class_embed_type: null
  cross_attention_dim: null
  learned_variance: true
  
  # Multi-Resolution Training Configuration
  multi_resolution:
    enabled: true                   # Enable progressive training
    strategy: "hierarchical"        # hierarchical, progressive, cascaded
    
    # Resolution curriculum - Progressive resolution schedule
    resolutions: [32, 64, 96, 128]  # Progressive resolution schedule [32, 64, 96, 128]
    base_resolution: 32             # Starting resolution
    target_resolution: 128          # Final target resolution
    
    # Progressive training features
    progressive:
      enabled: true                 # Use ProgressiveTrainer class
      max_resolution: 128           # Maximum resolution
      curriculum_type: "exponential" # linear, exponential, adaptive
      steps_per_resolution: [200000, 250000, 300000, 250000]  # Progressive steps: 32→64→96→128 (1M total)
      
      # Smooth transitions
      smooth_transitions: true      # Alpha-blending between resolutions
      transition_steps: 5000        # 5K steps for smooth transitions between resolutions
      blend_mode: "alpha"           # alpha, feature, loss
      
      # Adaptive features
      lr_scaling: true              # Scale learning rate with resolution
      lr_curriculum: "sqrt"         # sqrt, linear, constant
      batch_scaling: true           # Adaptive batch scaling
      adaptive_batch_scaling: true  # Memory-aware batch scaling

# Training configuration - Optimized for self-supervised microscopy learning
training:
  # Core training parameters - STEP-BASED TRAINING
  max_steps: 1000000            # 1 Million steps (much better for diffusion models)
  max_epochs: -1                # Disable epoch limit (step-based training)
  num_timesteps: 1000           # Diffusion timesteps (1000 is optimal for quality)
  beta_schedule: cosine         # cosine schedule works better than linear
  learning_rate: 7.5e-5         # Slightly lower LR for stability
  weight_decay: 1.0e-6          # L2 regularization
  
  # Batch and memory settings - SAFE for A40 memory limits
  batch_size: 48                # REDUCED: Safe batch size to avoid OOM (24GB usage)
  num_workers: 12               # REDUCED: Optimal for A40 (too many workers cause overhead)
  accumulate_grad_batches: 2    # ADDED: Effective batch = 48 * 2 = 96 (same effective size)
  gradient_clip: 1.0            # Gradient clipping for stability
  
  # Memory optimization - OPTIMIZED for A40 throughput
  persistent_workers: true      # Keep workers alive between epochs
  prefetch_factor: 4            # REDUCED: Less memory pressure, still good prefetch
  pin_memory: true              # Pin memory for faster GPU transfer
  drop_last: true               # Drop last incomplete batch for consistent timing
  
  # GPU Memory scaling guide for 128x128 images (proven values):
  # - A40/A100 (40-80GB): batch_size: 32 + accumulate_grad_batches: 3 (current, proven)
  # - RTX 4090 (24GB): batch_size: 24 + accumulate_grad_batches: 3
  # - RTX 3090 (24GB): batch_size: 16 + accumulate_grad_batches: 4
  # - RTX 3080 (10GB): batch_size: 8 + accumulate_grad_batches: 4
  # - RTX 3070 (8GB): batch_size: 4 + accumulate_grad_batches: 6
  
  # Model training enhancements
  use_ema: true                 # Exponential moving average for stable inference
  use_conditioning: false       # Disable WF conditioning for pure self-supervised learning
  conditioning_type: none       # No conditioning for grayscale microscopy

  # ELBO (variational bound) settings
  use_elbo_loss: true
  combine_elbo_with_simple: true
  elbo_weight: 1.0
  elbo_sigma0: 1.0
  elbo_observation: poisson    # poisson | gaussian
  elbo_poisson_epsilon: 1e-6
  learned_variance: true
  min_logvar: -10.0
  max_logvar: 2.0
  
  # ADAPTIVE LEARNING RATE SCHEDULING - Optimized for progressive training
  use_scheduler: true           # Enable adaptive learning rate
  scheduler_type: "improved_cosine"  # Advanced cosine with restarts
  scheduler_warmup_steps: 1000  # Warmup steps for learning rate
  
  # Advanced scheduler configuration
  advanced_schedulers:
    enabled: true               # Use SchedulerManager for dynamic switching
    primary_scheduler: "improved_cosine"
    scheduler_params:
      T_max: 50000             # Cosine period (50K steps) - matches resolution phases
      eta_min: 1e-6            # Minimum learning rate
      T_mult: 2                # Period multiplier for restarts
      restart_decay: 0.8       # Decay factor after each restart
    
    # Alternative schedulers for dynamic switching
    alternatives:
      - name: "exponential"
        type: "exponential"
        params:
          decay_rate: 0.96
      - name: "adaptive"
        type: "adaptive"
        params:
          adaptation_rate: 0.01
          patience: 10
          factor: 0.5
  
  # VALIDATION AND CHECKPOINTING - Best practices for diffusion training
  val_check_interval: 1.0      # Validate at end of each epoch (standard practice)
  log_every_n_steps: 50        # Log training metrics every 50 steps (more frequent)
  save_every_n_epochs: 5       # Save checkpoint every 5 epochs
  
  # PERFORMANCE OPTIMIZATIONS - A40 specific with memory safety
  precision: "16-mixed"         # Mixed precision for 2x speed boost
  compile_model: true           # PyTorch 2.0 compilation (15-20% speedup)
  use_fused_adam: true          # Fused AdamW optimizer
  benchmark: true               # Enable cuDNN benchmarking
  
  # MEMORY MANAGEMENT - Prevent OOM errors
  gradient_checkpointing: true  # Trade compute for memory (reduces memory usage)
  max_memory_usage: 0.85        # Use max 85% of GPU memory (safety margin)
  
  # EARLY STOPPING - More patient for dual objective training
  early_stopping_patience: 25     # 25 epochs patience (more reasonable)
  early_stopping_min_delta: 0.01  # Less sensitive to small fluctuations
  early_stopping_monitor: val_loss  # Metric to monitor
  early_stopping_mode: "min"   # Monitor for minimum loss
  early_stopping_verbose: true # Log early stopping decisions
  
  # DUAL OBJECTIVE TRAINING - disabled to focus on inference-time guidance
  use_dual_objective_loss: false
  
  # PURE SELF-SUPERVISED LEARNING - No supervised components
  supervised_x0_weight: 0.0     # DISABLED - Pure self-supervised learning
  ddpm_loss_weight: 1.0         # Self-supervised diffusion loss
  cycle_loss_weight: 0.15       # Self-supervised cycle consistency  
  perceptual_loss_weight: 0.0   # DISABLED - No supervised VGG features
  cycle_loss_type: smooth_l1    # smooth_l1, l1, or l2 (smooth_l1 is more robust)
  use_perceptual_loss: false    # DISABLED - Maintains self-supervised purity
  
  # NOTE: Validation and logging settings moved to top of training section
  
  # COMPREHENSIVE MODEL SAVING STRATEGY
  checkpoint_every_n_steps: [5000, 10000]  # Multiple checkpoint intervals
  save_intermediate_checkpoints: true      # Save at both intervals
  
  # Advanced checkpoint configuration
  checkpoint_config:
    # Main checkpoints (every 10K steps)
    main_checkpoint:
      every_n_train_steps: 10000    # Main checkpoint frequency
      save_top_k: 5                 # Keep best 5 models
      monitor: "val/loss"           # Monitor validation loss
      mode: "min"                   # Save minimum loss
      filename: "ddpm-{step:06d}-{val/loss:.4f}"
      save_last: true               # Always save latest
      
    # Intermediate checkpoints (every 5K steps) 
    intermediate_checkpoint:
      every_n_train_steps: 5000     # Recovery checkpoint frequency
      save_top_k: -1                # Save all intermediate checkpoints
      filename: "ddmp-intermediate-{step:06d}"
      save_last: false              # Don't duplicate latest
      
    # Progressive resolution checkpoints
    resolution_checkpoints: true    # Save at each resolution transition
    save_best_per_resolution: true  # Best model for each resolution phase
  
  # Advanced training options
  steps_per_epoch: 263          # Corrected: 33,635 samples ÷ 128 effective batch size = 263 steps/epoch
  
  # PERFORMANCE OPTIMIZATIONS - Maximize A40 utilization
  dataloader_pin_memory_device: "cuda"  # Pin directly to GPU
  
# Data configuration - 128x128 patches with 16-bit support
data:
  image_size: 128               # 128x128 patches from frame-based splits
  min_intensity: 0              # Minimum intensity value (16-bit)
  max_intensity: 65535          # Maximum intensity value (16-bit range)
  noise_model: poisson          # Poisson noise model for microscopy
  use_16bit_normalization: true # Proper 16-bit normalization
  align_pairs: true             # Align WF to 2P images
  use_self_supervised: true     # Use self-supervised learning with forward model
  use_zarr: false               # Set to true for very large datasets
  
  # Multi-resolution Data Pipeline
  data_loading:
    # Standard data loading
    use_zarr: false               # Use standard image loading
    cache_dataset: true           # Enable dataset caching for efficiency
    
    # Multi-resolution data pipeline
    multi_resolution:
      enabled: true               # Use ProgressiveDataLoader
      precompute_scales: true     # Pre-compute resolution versions
      interpolation_method: "lanczos"  # High-quality interpolation
      preserve_aspect_ratio: true # Maintain aspect ratios
      
      # Resolution-specific augmentations
      scale_specific_augmentation:
        enabled: true
        low_res_augmentations:    # Stronger augmentation for low-res
          noise_std: 0.1
          blur_probability: 0.3
        high_res_augmentations:   # Gentler augmentation for high-res
          noise_std: 0.05
          blur_probability: 0.1
  
  # Generalized Anscombe Transform parameters (for poisson_gaussian noise)
  gat:
    alpha: 1.0                  # Poisson scaling factor
    mu: 0.0                     # Gaussian mean
    sigma: 0.0                  # Gaussian standard deviation

# PSF configuration for self-supervised cycle consistency - 2μm bead
psf:
  type: gaussian                # Type of PSF (gaussian, file, measured)
  sigma_x: 2.0                  # Standard deviation in x direction (2μm bead)
  sigma_y: 2.0                  # Standard deviation in y direction (2μm bead)
  size: 21                      # PSF kernel size (should be odd)
  background: 0.0               # Background intensity level

# Physics configuration
physics:
  use_psf: true
  use_bead_psf: true
  beads_dir: ${paths.data}/beads  # Use 2μm bead PSF from data/real_microscopy/beads/
  bead_mode: with_AO  # Use the AO-corrected PSF
  psf_type: measured  # Use measured PSF from beads
  background: 0.0
  noise_type: poisson

# Guidance configuration
guidance:
  type: pkl                     # pkl, l2, anscombe
  epsilon: 1e-6                 # Guidance strength
  schedule_type: adaptive       # adaptive, linear, constant
  lambda_base: 0.1              # Base guidance weight
  schedule:
    T_threshold: 800            # Threshold for adaptive scheduling
    epsilon_lambda: 1e-3        # Adaptive scheduling parameter

# Evaluation configuration
evaluation:
  metrics: ["psnr", "ssim", "frc"]  # Metrics to compute
  save_samples: true            # Save sample outputs during training
  num_samples: 8                # Number of samples to save
  eval_every_n_epochs: 5        # Evaluate every N epochs

# Inference configuration
inference:
  ddim_steps: 50                # Number of DDIM sampling steps
  eta: 0.0                      # DDIM eta parameter (0.0 = deterministic)
  use_autocast: true            # Use automatic mixed precision
  batch_size: 4                 # Inference batch size
  guidance_scale: 1.0           # Guidance scale for sampling

# Hardware optimization - Single A40 GPU setup
hardware:
  devices: 1                    # Single A40 GPU
  accelerator: gpu              # gpu, cpu, tpu
  strategy: auto                # Auto strategy for single GPU
  sync_batchnorm: false         # Not needed for single GPU
  
# Logging and monitoring
logging:
  log_level: INFO               # DEBUG, INFO, WARNING, ERROR
  log_every_n_steps: 100        # Log frequency
  save_top_k: 3                 # Keep top K checkpoints
  monitor_metric: val_loss      # Metric to monitor for best checkpoint
  
# Memory and performance optimization - MAXIMIZED for A40
optimization:
  channels_last: true           # Use channels last memory format (better A40 performance)
  benchmark_cudnn: true         # Enable cuDNN benchmarking
  deterministic: false          # Non-deterministic for maximum speed
  
  # Advanced memory optimization for A40
  memory_efficient_attention: true    # Use memory efficient attention
  gradient_checkpointing: true        # ENABLED: Trade compute for memory (allows larger batches)
  use_gradient_checkpointing: true    # Enable gradient checkpointing in training
  use_tf32: true                      # Enable TF32 on A40 for speed
  torch_compile_mode: "max-autotune"  # Maximum optimization
  
# MICROSCOPY-SPECIFIC DATA AUGMENTATION - Optimized for dual objectives
augmentation:
  enabled: true                 # ENABLED: Essential for robust microscopy training
  
  # Geometric augmentations (microscopy-safe)
  horizontal_flip: 0.5          # Safe for microscopy (no directional bias)
  vertical_flip: 0.5            # Safe for microscopy
  rotation: 0.1                 # Small rotations (preserve microscopy structures)
  
  # Advanced microscopy augmentations
  random_crop: true             # Random cropping with scale
  crop_scale: [0.9, 1.0]        # Conservative cropping (preserve structures)
  elastic_transform: true       # Excellent for biological images
  elastic_alpha: 20.0           # Moderate deformation (preserve biology)
  elastic_sigma: 3.0            # Smooth deformation
  
  # INTENSITY AUGMENTATIONS - Critical for dual objective training
  intensity_augmentation:
    enabled: true               # Essential for intensity mapping
    brightness: 0.05            # Subtle brightness changes (preserve signal)
    contrast: 0.05              # Subtle contrast changes
    gamma: [0.95, 1.05]         # Small gamma corrections
    
    # Microscopy-specific intensity augmentation
    intensity_scale_range: [0.98, 1.02]  # Preserve signal relationships
    noise_injection: true       # Add small noise to increase variation
    noise_std: 0.01             # Small noise (increase 2P variation)
    poisson_noise: true         # Microscopy-appropriate noise model
  
  # Quality control
  augment_probability: 0.8      # High probability for robust training
  preserve_intensity_mapping: true  # Critical for dual objectives
  
# Resume training (optional)
# resume_checkpoint: null       # Path to checkpoint to resume from
# resume_start_epoch: 0         # Epoch to start from when resuming
